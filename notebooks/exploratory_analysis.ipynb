{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison for MNIST\n",
    "\n",
    "This notebook evaluates and compares the performance of three different classifiers implemented from scratch:\n",
    "1. **Softmax Regression**\n",
    "2. **Neural Network (Baseline)**\n",
    "3. **Neural Network (Tuned)**\n",
    "\n",
    "We will use the utility functions from `src/utils.py` to generate reports, confusion matrices, and other visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msoftmax_regression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SoftmaxRegression\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneural_network\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuralNetwork\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_model, plot_learning_curves\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading data...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Nayan\\Projects\\Multiclassification-from-scratch\\notebooks\\..\\src\\utils.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report, accuracy_score\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..') # Add the project root to the path\n",
    "\n",
    "# Import our models and utility functions\n",
    "from src.data_loader import load_mnist\n",
    "from src.softmax_regression import SoftmaxRegression\n",
    "from src.neural_network import NeuralNetwork\n",
    "from src.utils import evaluate_model, plot_learning_curves\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "X_train, X_test, y_train, y_test = load_mnist()\n",
    "\n",
    "# Define class names for plotting\n",
    "class_names = [str(i) for i in range(10)]\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 0: One-vs-Rest Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logistic_regression import LogisticRegression, OneVsRestClassifier\n",
    "\n",
    "# Initialize and train the OvR model.\n",
    "\n",
    "lr_base = LogisticRegression(learning_rate=0.1, n_iterations=1000, regularization_lambda=0.01)\n",
    "ovr_model = OneVsRestClassifier(base_classifier=lr_base, n_classes=10)\n",
    "\n",
    "# The fit method in this class already prints progress, so we just call it.\n",
    "ovr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(ovr_model, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment 1: Softmax Regression\n",
    "\n",
    "First, we evaluate the Softmax Regression model. It's a strong linear baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Softmax Regression model\n",
    "softmax_model = SoftmaxRegression(learning_rate=0.1, n_iterations=1000, regularization_lambda=0.01)\n",
    "softmax_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(softmax_model, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The Softmax model provides a solid baseline accuracy. The confusion matrix shows which digits are commonly confused (e.g., 4 vs. 9, 3 vs. 5). Being a linear model, its ability to separate complex patterns is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 2: Neural Network (Baseline)\n",
    "\n",
    "Now, let's evaluate the simple feed-forward neural network with its default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the baseline Neural Network\n",
    "nn_baseline = NeuralNetwork(learning_rate=0.001, n_epochs=10, batch_size=64)\n",
    "history_baseline = nn_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Plot its learning curve\n",
    "plot_learning_curves(history_baseline, title=\"Baseline NN Training Loss\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(nn_baseline, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The baseline neural network significantly outperforms the softmax model, demonstrating the power of non-linear hidden layers. The learning curve shows a steady decrease in loss. The accuracy is much higher, and the number of misclassifications in the confusion matrix is visibly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 3: Hyperparameter Tuning the Neural Network\n",
    "\n",
    "Let's try to improve the neural network. We will train it for more epochs and use a slightly smaller learning rate to allow for finer convergence. This is a common tuning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the tuned Neural Network\n",
    "nn_tuned = NeuralNetwork(learning_rate=0.0005, n_epochs=20, batch_size=64)\n",
    "history_tuned = nn_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Plot its learning curve\n",
    "plot_learning_curves(history_tuned, title=\"Tuned NN Training Loss\")\n",
    "\n",
    "# Evaluate the tuned model\n",
    "evaluate_model(nn_tuned, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Based on the evaluations:\n",
    "\n",
    "- The **Neural Network** architecture is far superior to the linear **Softmax Regression** model for this image classification task.\n",
    "- **Hyperparameter tuning** (adjusting epochs and learning rate) provided a noticeable boost in accuracy for the Neural Network.\n",
    "- The visualization tools in `utils.py` were crucial for diagnosing model performance, comparing results, and understanding where the models fail (e.g., via the confusion matrix and incorrect prediction plots)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
